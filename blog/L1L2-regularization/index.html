<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Blog - Lee Kai Han Portfolio</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="/assets/img/favicon.png" rel="icon">
  <link href="/assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="/assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="/assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="/assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="/assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="/assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="/assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="/assets/css/style.css" rel="stylesheet">

  <!-- See http://docs.mathjax.org/en/latest/web/start.html#using-mathjax-from-a-content-delivery-network-cdn -->
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>

    <!-- ======= Mobile nav toggle button ======= -->
    <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

    <!-- ======= Header ======= -->
    <header id="header">
        <div class="d-flex flex-column">

            <div class="profile">
                <img src="/assets/img/profile-image.jpg" alt="" class="img-fluid rounded-circle">
                <h1 class="text-light"><a href="/#">Lee Kai Han</a></h1>
                <div class="social-links mt-3 text-center">
                    <a href="https://www.linkedin.com/in/kaihanlee/" class="linkedin" target="_blank"><i class="bx bxl-linkedin"></i></a>
                    <a href="https://github.com/kaihanlee" class="github" target="_blank"><i class="bx bxl-github"></i></a>

                </div>
            </div>

            <nav id="navbar" class="nav-menu navbar">
                <ul>
                    <li><a href="/#"><i class="bx bx-home"></i> <span>Home</span></a></li>
                    <li><a href="/about"><i class="bx bx-user"></i> <span>About Me</span></a></li>
                    <li><a href="/experiences"><i class="bx bx-file-blank"></i> <span>Experiences</span></a></li>
                    <li><a href="/projects"><i class="bx bx-book-content"></i> <span>Projects</span></a></li>
                    <li><a href="/blog"><i class="bx bx-pencil"></i> <span>Blog</span></a></li>
                    <li><a href="/contact"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
                </ul>
            </nav><!-- .nav-menu -->
        </div>
    </header><!-- End Header -->

    <main id="main">

        <!-- ======= Portfolio Section ======= -->
        <section id="portfolio" class="portfolio section-bg">
            <div class="container">

                <div class="section-title">
                    <h1>L1 vs L2 Regularization</h1>
                    <em>21 October 2021, 5 mins read</em>
                    <br><br>

                    <p>
                      The basic regression formula:
                      $$Y = a + b_1X_1 + b_2X_2 + \dots + \epsilon$$
                    </p><br>

                    <h3>Regularization</h3>
                    <p>Regularization is used in supervised machine learning models. This is used by adding regularization terms to the loss function of the model. The objective of regularization is to improve the machine learning process by avoiding overfitting by introducing bias (a.k.a. regularization term) to the model in order to produce a model that can generate output with a far lower variance. </p><br>
                    <p>Regularization is essential because ML models can perform well on training data but perform badly on test data due to multiple factors, such as bias-variance impact due to overfitting. </p><br>
                    <p>To understand regularization, one needs to understand regression analysis, where the features are estimated using coefficients. The newly introduced regularization term allows for the coefficients to be penalized which generalizes the model in order to reduce overfitting. </p><br>
                    <p>There are two types of regularizations, L1 Regularization which is used in Lasso Regression, and L2 Regularization which is used in Ridge Regression.</p><br>

                    <h3>Bias-Variance Trade-Off</h3>
                    <p>Let \lambda be the penalty term. As \lambda increases, bias increases, variance decreases. This trend needs to be utilized in order to make regularization work. We will increase \lambda to increase bias of the model, directly reducing overfitting on the training data, hence decreasing variance on the test data and the model. </p><br>

                    <h3>Assumptions</h3>
                    <p>As usual, the observations are independent and identically distributed, have constant variance. </p><br>
                    <p>It is worth highlighting that when sum of squared residuals is mentioned, it is calculated with the ordinary least squares method (constraint on y). </p><br>

                    <h3>L1 Regularization (Lasso Regression)</h3>
                    <p>L1 regularization is used when there is a high amount of features which needs to be reduced via feature selection. <a href="https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models">Here's a direct explanation on how L1 enforces sparsity. </a>Lasso regression minimizes the sum of squared residuals plus the product of \lambda and the absolute value of weights of coefficients, which is the lasso regression regularization term.</p>
                    <p>$$Cost = \sum^N_{i=0}(y_i - \sum^M_{j=0}x_{ij}W_j)^2 + \lambda \sum^M_{j=0}|W_j|$$</p><br>

                    <h3>L2 Regularization (Ridge Regression)</h3>
                    <p>L2 regularization is used when independent variables are highly correlated (strong multicollinearity). Ridge regression minimizes the sum of squared residuals plus the product of \lambda and the squared weights of coefficients, which is the ridge regression regularization term. </p>
                    <p>$$Cost = \sum^N_{i=0}(y_i - \sum^M_{j=0}x_{ij}W_j)^2 + \lambda \sum^M_{j=0}W_j^2$$</p><br>

                    <h3>On Distinct Data and Logistic Regression</h3>
                    <p>L1 and L2 regularizations work on distinct variables and logistic regression too. The regression formula will be \(Y = a + bX\), where \(a\) is the weighted average of the first variable and \(b\) is the coefficient for the second variable. </p><br>
                    <p>Both regressions minimize the sum of squared residuals plus the product of \lambda and the squared weights of coefficients (in this case, \(b^2\)). We can include as many coefficients as we have. However, the coefficients do not have to be shrunk at the same pace. </p><br>
                    <p>In the case of logistic regression, both regressions minimize the sum of likelihoods instead of the squared residuals as logistic regression is solved using maximum likelihood.</p><br>

                    <h3>Pros and Cons</h3>
                    <p>We use ridge regression because when the training data is small, the model will overfit very easily. With ridge regression, we introduce bias to the model and indirectly reduce the variance of the output. </p><br>
                    <p>Ridge regression can shrink parameters asymptotically close to zero but not down to absolute zero. Lasso regression can totally eliminate parameters (or shrink them down to absolute zero), this is useful to reduce variance in models that have many useless variables.</p><br>

                    <h3>Conclusion</h3>
                    <p>Regularization is a common mathematical technique to prevent overfitting, it does so by introducing a regularization term to the loss function. It depends on the objective and the constraints of the analysis, L1 or L2 regularization can be useful in different ways.</p>

                </div>

            </div>
        </section><!-- End Portfolio Section -->

    </main><!-- End #main -->

    <!-- ======= Footer ======= -->
    <footer id="footer">
        <div class="container">
            <div class="credits">
                <span>&copy</span> Designed by <strong>Lee Kai Han</strong>
                <div class="credits">
                    Powered by BootstrapMade
                </div>
            </div>
        </div>
    </footer><!-- End  Footer -->

    <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

    <!-- Vendor JS Files -->
    <script src="/assets/vendor/aos/aos.js"></script>
    <script src="/assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="/assets/vendor/glightbox/js/glightbox.min.js"></script>
    <script src="/assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="/assets/vendor/php-email-form/validate.js"></script>
    <script src="/assets/vendor/purecounter/purecounter.js"></script>
    <script src="/assets/vendor/swiper/swiper-bundle.min.js"></script>
    <script src="/assets/vendor/typed.js/typed.min.js"></script>
    <script src="/assets/vendor/waypoints/noframework.waypoints.js"></script>

    <!-- Template Main JS File -->
    <script src="/assets/js/main.js"></script>

</body>

</html>
